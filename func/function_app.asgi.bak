import azure.functions as func 
from fastapi import FastAPI, Request, Response, Query, Depends
from fastapi.responses import StreamingResponse
from typing import Union, Any, Callable, Coroutine

import logging
import os
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from openai import AsyncAzureOpenAI
import json
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

from azure.monitor.events.extension import track_event
from azure.monitor.opentelemetry import configure_azure_monitor
from opentelemetry import trace
from opentelemetry import metrics
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator
import asyncio
import time
from contextlib import asynccontextmanager
from azure.core.settings import settings
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.asgi import OpenTelemetryMiddleware

settings.tracing_implementation = "opentelemetry"

configured_azure_monitor = False

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)
meter = metrics.get_meter_provider().get_meter("openaifunction")
token_counter = meter.create_counter("openaitokens")

@asynccontextmanager
async def lifespan(app: FastAPI):
    global configured_azure_monitor
    logging.info("appinsights_setup...")
    if not configured_azure_monitor:
        # configure_azure_monitor(
        #     connection_string=os.environ.get("APPLICATIONINSIGHTS_CONNECTION_STRING"),
        # )
        configured_azure_monitor = True
    yield
    


fast_app = FastAPI(lifespan=lifespan) 

@fast_app.middleware("http")
async def my_custom_middleware(request: Request, call_next):
    start_time = time.time()
    logging.info("my custom middleware")
    logging.info(f"request: {request}")
    logging.info(f"headers: {request.headers}")
    logging.info(f"request state: {request.state}")
    logging.info(f"request scope: {request.scope}")
    logging.info(f"trace context: {request.scope.get('azure_functions.trace_context', '')}")
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-My-Process-Time"] = str(process_time)
    logging.info(f"X-My-Process-Time: {process_time}")
    response.headers["X-AF-TC"] = request.scope.get('azure_functions.trace_context').Traceparent
    logging.info(f"X-AF-TC: {request.scope.get('azure_functions.trace_context').Traceparent}")
    return response


@fast_app.get("/return_http_no_body") 
async def return_http_no_body(): 
    return Response(content="", media_type="text/plain") 

@fast_app.get("/hello") 
async def hello(): 
    return Response(content="Hello World!", media_type="text/plain") 



async def get_streamingresponse_openai(prompt):
    engine = os.environ.get("ENGINE")

    # Request credential
    token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")

    # Setup parameters
    openai_client = AsyncAzureOpenAI(
        azure_endpoint= os.environ.get("API_BASE"),
        azure_ad_token_provider=token_provider,
        api_version = "2023-10-01-preview"
    )

    stream = await openai_client.chat.completions.create(
            model=engine,
            messages=[
                {"role": "system", "content": 'You are an AI assistant that helps people find information about "Star Wars".\n\nInstructions\n- only answer questions related to Star Wars\n- If an answer is not related to Star Wars, respond with "This is not the AI you are looking for..."'},
                {"role": "user", "content": prompt},
            ],
            stream = True,
        )
    
    async for chunk in stream:
        logging.info(chunk)
        if len(chunk.choices) > 0:
            if chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content

@fast_app.get("/openaistreaming")
async def get_openai_streaming(request: Request, question: Union[str, None] = Query(default="", max_length=50)):
    return StreamingResponse(get_streamingresponse_openai(question), media_type="text/event-stream")

@fast_app.get("/openai")
async def get_openai(request: Request, question: Union[str, None] = Query(default="", max_length=50)):
    # Request credential
    token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")

    # Setup parameters
    openai_client = AsyncAzureOpenAI(
        azure_endpoint= os.environ.get("API_BASE"),
        azure_ad_token_provider=token_provider,
        api_version = "2023-10-01-preview"
    )

    engine = os.environ.get("ENGINE")
    traceparent = request.scope.get('azure_functions.trace_context').Traceparent
    operation_id = traceparent.split("-")[1]
    
    w3c_trace_context = { "traceparent": traceparent }
    logging.info(f"w3c_trace_context = {w3c_trace_context}")
    ctx = TraceContextTextMapPropagator().extract(carrier=w3c_trace_context)
    with tracer.start_as_current_span("openai-function", context=ctx):
        
        user_prompt = "What is blue milk?"
        if question:
            user_prompt = question

        response = await openai_client.chat.completions.create(
            model=engine,
            messages=[
                {"role": "system", "content": 'You are an AI assistant that helps people find information about "Star Wars".\n\nInstructions\n- only answer questions related to Star Wars\n- If an answer is not related to Star Wars, respond with "This is not the AI you are looking for..."'},
                {"role": "user", "content": user_prompt},
            ],
        )

        logging.info(response)

        token_counter.add(response.usage.total_tokens, { "function": os.environ.get("OTEL_SERVICE_NAME"), "operation_id": operation_id, "streaming": "false" })
        track_event("openai-tokens", {"function": os.environ.get("OTEL_SERVICE_NAME"), "total_tokens": response.usage.total_tokens, "operation_id": operation_id , "streaming": "false"})
        return Response(content=response.model_dump_json(indent=2), media_type="application/json")

FastAPIInstrumentor.instrument_app(fast_app)
app = func.AsgiFunctionApp(app=fast_app, 
                           http_auth_level=func.AuthLevel.FUNCTION) 
